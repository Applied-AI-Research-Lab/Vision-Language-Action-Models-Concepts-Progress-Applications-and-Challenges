# Paper List 2024

### Follow-up Papers

- **chang2024survey:** Chang, Yupeng, Wang, Xu, Wang, Jindong, Wu, Yuan, Yang, Linyi, Zhu, Kaijie, Chen, Hao, Yi, Xiaoyuan, Wang, Cunxiang, Wang, Yidong, others.<br />
  "A survey on evaluation of large language models." ACM transactions on intelligent systems and technology (2024).

  [2024]

- **chen2024spatialvlm:** Chen, Boyuan, Xu, Zhuo, Kirmani, Sean, Ichter, Brain, Sadigh, Dorsa, Guibas, Leonidas, Xia, Fei.<br />
  "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024).

  [2024]

- **ma2024survey:** Ma, Yueen, Song, Zixing, Zhuang, Yuzheng, Hao, Jianye, King, Irwin.<br />
  "A survey on vision-language-action models for embodied ai." arXiv preprint arXiv:2405.14093 (2024).

  [2024]

- **luo2024precise:** Luo, Jianlan, Xu, Charles, Wu, Jeffrey, Levine, Sergey.<br />
  "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning." arXiv preprint arXiv:2410.21845 (2024).

  [2024]

- **jeong2024survey:** Jeong, Hyeongyo, Lee, Haechan, Kim, Changwon, Shin, Sungtae.<br />
  "A survey of robot intelligence with large language models." Applied Sciences (2024).

  [2024]

- **team2024octo:** Team, Octo Model, Ghosh, Dibya, Walke, Homer, Pertsch, Karl, Black, Kevin, Mees, Oier, Dasari, Sudeep, Hejna, Joey, Kreiman, Tobias, Xu, Charles, others.<br />
  "Octo: An open-source generalist robot policy." arXiv preprint arXiv:2405.12213 (2024).

  [2024]

- **kim2024openvla:** Kim, Moo Jin, Pertsch, Karl, Karamcheti, Siddharth, Xiao, Ted, Balakrishna, Ashwin, Nair, Suraj, Rafailov, Rafael, Foster, Ethan, Lam, Grace, Sanketi, Pannag, others.<br />
  "Openvla: An open-source vision-language-action model." arXiv preprint arXiv:2406.09246 (2024).

  [2024]

- **black2024pi_0:** Black, Kevin, Brown, Noah, Driess, Danny, Esmail, Adnan, Equi, Michael, Finn, Chelsea, Fusai, Niccolo, Groom, Lachy, Hausman, Karol, Ichter, Brian, others.<br />
  "Pi-0: A Vision-Language-Action Flow Model for General Robot Control." arXiv preprint arXiv:2410.24164 (2024).

  [2024]

- **liu2024rdt:** Liu, Songming, Wu, Lingxuan, Li, Bangguo, Tan, Hengkai, Chen, Huayu, Wang, Zhengyi, Xu, Ke, Su, Hang, Zhu, Jun.<br />
  "Rdt-1b: a diffusion foundation model for bimanual manipulation." arXiv preprint arXiv:2410.07864 (2024).

  [2024]

- **li2024cogact:** Li, Qixiu, Liang, Yaobo, Wang, Zeyu, Luo, Lin, Chen, Xi, Liao, Mozheng, Wei, Fangyun, Deng, Yu, Xu, Sicheng, Zhang, Yizhong, others.<br />
  "Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation." arXiv preprint arXiv:2411.19650 (2024).

  [2024]

- **xu2024survey:** Xu, Zhiyuan, Wu, Kun, Wen, Junjie, Li, Jinming, Liu, Ning, Che, Zhengping, Tang, Jian.<br />
  "A survey on robotics with foundation models: toward embodied ai." arXiv preprint arXiv:2402.02385 (2024).

  [2024]

- **li2024improving:** Li, Jinming, Zhu, Yichen, Tang, Zhibin, Wen, Junjie, Zhu, Minjie, Liu, Xiaoyu, Li, Chengmeng, Cheng, Ran, Peng, Yaxin, Feng, Feifei.<br />
  "Improving Vision-Language-Action Models via Chain-of-Affordance." arXiv preprint arXiv:2412.20451 (2024).

  [2024]

- **budzianowski20edgevla:** Budzianowski, Pawe{\l}, Maa, Wesley, Freed, Matthew, Mo, Jingxiang, Xie, Aaron, Tipnis, Viraj, Bolte, Benjamin.<br />
  "EdgeVLA: Efficient Vision-Language-Action Models." environments (2024).

  [2024]

- **lin2024showui:** Lin, Kevin Qinghong, Li, Linjie, Gao, Difei, Yang, Zhengyuan, Wu, Shiwei, Bai, Zechen, Lei, Weixian, Wang, Lijuan, Shou, Mike Zheng.<br />
  "Showui: One vision-language-action model for gui visual agent." arXiv preprint arXiv:2411.17465 (2024).

  [2024]

- **wen2024diffusion:** Wen, Junjie, Zhu, Minjie, Zhu, Yichen, Tang, Zhibin, Li, Jinming, Zhou, Zhongyi, Li, Chengmeng, Liu, Xiaoyu, Peng, Yaxin, Shen, Chaomin, others.<br />
  "Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression." arXiv preprint arXiv:2412.03293 (2024).

  [2024]

- **cao2024ai:** Cao, Longbing.<br />
  "Ai robots and humanoid ai: Review, perspectives and directions." arXiv preprint arXiv:2405.15775 (2024).

  [2024]

- **ding2024quar:** Ding, Pengxiang, Zhao, Han, Zhang, Wenjie, Song, Wenxuan, Zhang, Min, Huang, Siteng, Yang, Ningxi, Wang, Donglin.<br />
  "Quar-vla: Vision-language-action model for quadruped robots." European Conference on Computer Vision (2024).

  [2024]

- **wang2024towards:** Wang, Zhijie, Zhou, Zhehua, Song, Jiayang, Huang, Yuheng, Shu, Zhan, Ma, Lei.<br />
  "Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study." arXiv preprint arXiv:2409.12894 (2024).

  [2024]

- **gbagbe2024bi:** Gbagbe, Koffivi Fid{\`e}le, Cabrera, Miguel Altamirano, Alabbas, Ali, Alyunes, Oussama, Lykov, Artem, Tsetserukou, Dzmitry.<br />
  "Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations." 2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC) (2024).

  [2024]

- **liu2024robomamba:** Liu, Jiaming, Liu, Mengzhen, Wang, Zhenyu, An, Pengju, Li, Xiaoqi, Zhou, Kaichen, Yang, Senqiao, Zhang, Renrui, Guo, Yandong, Zhang, Shanghang.<br />
  "Robomamba: Efficient vision-language-action model for robotic reasoning and manipulation." Advances in Neural Information Processing Systems (2024).

  [2024]

- **cheng2024navila:** Cheng, An-Chieh, Ji, Yandong, Yang, Zhaojing, Gongye, Zaitian, Zou, Xueyan, Kautz, Jan, B{\i}y{\i}k, Erdem, Yin, Hongxu, Liu, Sifei, Wang, Xiaolong.<br />
  "Navila: Legged robot vision-language-action model for navigation." arXiv preprint arXiv:2412.04453 (2024).

  [2024]

- **li2024robonurse:** Li, Shunlei, Wang, Jin, Dai, Rui, Ma, Wanyu, Ng, Wing Yin, Hu, Yingbai, Li, Zheng.<br />
  "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model." arXiv preprint arXiv:2409.19590 (2024).

  [2024]

- **chiang2024mobility:** Chiang, Hao-Tien Lewis, Xu, Zhuo, Fu, Zipeng, Jacob, Mithun George, Zhang, Tingnan, Lee, Tsang-Wei Edward, Yu, Wenhao, Schenck, Connor, Rendleman, David, Shah, Dhruv, others.<br />
  "Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs." arXiv preprint arXiv:2407.07775 (2024).

  [2024]

- **yue2024deer:** Yue, Yang, Wang, Yulin, Kang, Bingyi, Han, Yizeng, Wang, Shenzhi, Song, Shiji, Feng, Jiashi, Huang, Gao.<br />
  "Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution." Advances in Neural Information Processing Systems (2024).

  [2024]

- **zhang2024uni:** Zhang, Jiazhao, Wang, Kunyu, Wang, Shaoan, Li, Minghan, Liu, Haoran, Wei, Songlin, Wang, Zhongyuan, Zhang, Zhizheng, Wang, He.<br />
  "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks." arXiv preprint arXiv:2412.06224 (2024).

  [2024]

- **wei2024occllama:** Wei, Julong, Yuan, Shanshuai, Li, Pengfei, Hu, Qingda, Gan, Zhongxue, Ding, Wenchao.<br />
  "Occllama: An occupancy-language-action generative world model for autonomous driving." arXiv preprint arXiv:2409.03272 (2024).

  [2024]

- **dey2024revla:** Dey, Sombit, Zaech, Jan-Nico, Nikolov, Nikolay, Van Gool, Luc, Paudel, Danda Pani.<br />
  "Revla: Reverting visual domain limitation of robotic foundation models." arXiv preprint arXiv:2409.15250 (2024).

  [2024]

- **wu2024visionllm:** Wu, Jiannan, Zhong, Muyan, Xing, Sen, Lai, Zeqiang, Liu, Zhaoyang, Chen, Zhe, Wang, Wenhai, Zhu, Xizhou, Lu, Lewei, Lu, Tong, others.<br />
  "Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks." Advances in Neural Information Processing Systems (2024).

  [2024]

- **han2024review:** Han, Songyue, Wang, Mingyu, Zhang, Jialong, Li, Dongdong, Duan, Junhong.<br />
  "A Review of Large Language Models: Fundamental Architectures, Key Technological Evolutions, Interdisciplinary Technologies Integration, Optimization and Compression Techniques, Applications, and Challenges." Electronics (2024).

  [2024]

- **ni2024peria:** Ni, Fei, Hao, Jianye, Wu, Shiguang, Kou, Longxin, Yuan, Yifu, Dong, Zibin, Liu, Jinyi, Li, MingZhi, Zhuang, Yuzheng, Zheng, Yan.<br />
  "Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation." Advances in Neural Information Processing Systems (2024).

  [2024]

- **xiong2024autoregressive:** Xiong, Jing, Liu, Gongye, Huang, Lun, Wu, Chengyue, Wu, Taiqiang, Mu, Yao, Yao, Yuan, Shen, Hui, Wan, Zhongwei, Huang, Jinfa, others.<br />
  "Autoregressive Models in Vision: A Survey." arXiv preprint arXiv:2411.05902 (2024).

  [2024]

- **lu2024unified:** Lu, Jiasen, Clark, Christopher, Lee, Sangho, Zhang, Zichen, Khosla, Savya, Marten, Ryan, Hoiem, Derek, Kembhavi, Aniruddha.<br />
  "Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024).

  [2024]

- **tian2024visual:** Tian, Keyu, Jiang, Yi, Yuan, Zehuan, Peng, Bingyue, Wang, Liwei.<br />
  "Visual autoregressive modeling: Scalable image generation via next-scale prediction." Advances in neural information processing systems (2024).

  [2024]

- **bordes2024introduction:** Bordes, Florian, Pang, Richard Yuanzhe, Ajay, Anurag, Li, Alexander C, Bardes, Adrien, Petryk, Suzanne, Ma{\~n}as, Oscar, Lin, Zhiqiu, Mahmoud, Anas, Jayaraman, Bargav, others.<br />
  "An introduction to vision-language modeling." arXiv preprint arXiv:2405.17247 (2024).

  [2024]

- **li2024foundation:** Li, Dingzhe, Jin, Yixiang, Sun, Yuhao, Yu, Hongze, Shi, Jun, Hao, Xiaoshuai, Hao, Peng, Liu, Huaping, Sun, Fuchun, Zhang, Jianwei, others.<br />
  "What foundation models can bring for robot learning in manipulation: A survey." arXiv preprint arXiv:2404.18201 (2024).

  [2024]

- **ding2024understanding:** Ding, Jingtao, Zhang, Yunke, Shang, Yu, Zhang, Yuheng, Zong, Zefang, Feng, Jie, Yuan, Yuan, Su, Hongyuan, Li, Nian, Sukiennik, Nicholas, others.<br />
  "Understanding World or Predicting Future? A Comprehensive Survey of World Models." arXiv preprint arXiv:2411.14499 (2024).

  [2024]

- **ghosh2024exploring:** Ghosh, Akash, Acharya, Arkadeep, Saha, Sriparna, Jain, Vinija, Chadha, Aman.<br />
  "Exploring the frontier of vision-language models: A survey of current methodologies and future directions." arXiv preprint arXiv:2404.07214 (2024).

  [2024]

- **huangearly:** Huang, Huang, Liu, Fangchen, Fu, Letian, Wu, Tingfan, Mukadam, Mustafa, Malik, Jitendra, Goldberg, Ken, Abbeel, Pieter.<br />
  "Early Fusion Helps Vision Language Action Models Generalize Better." 1st Workshop on X-Embodiment Robot Learning (2024).

  [2024]

- **agarwal2024methods:** Agarwal, Lakshita, Verma, Bindu.<br />
  "From methods to datasets: A survey on Image-Caption Generators." Multimedia Tools and Applications (2024).

  [2024]

- **sameni2024building:** Sameni, Sepehr, Kafle, Kushal, Tan, Hao, Jenni, Simon.<br />
  "Building vision-language models on solid foundations with masked distillation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024).

  [2024]

- **moroncelli2024integrating:** Moroncelli, Angelo, Soni, Vishal, Shahid, Asad Ali, Maccarini, Marco, Forgione, Marco, Piga, Dario, Spahiu, Blerina, Roveda, Loris.<br />
  "Integrating reinforcement learning with foundation models for autonomous robotics: Methods and perspectives." arXiv preprint arXiv:2410.16411 (2024).

  [2024]

- **belkhale2024rt:** Belkhale, Suneel, Ding, Tianli, Xiao, Ted, Sermanet, Pierre, Vuong, Quon, Tompson, Jonathan, Chebotar, Yevgen, Dwibedi, Debidatta, Sadigh, Dorsa.<br />
  "Rt-h: Action hierarchies using language." arXiv preprint arXiv:2403.01823 (2024).

  [2024]

- **foster2024behavior:** Foster, Dylan J, Block, Adam, Misra, Dipendra.<br />
  "Is behavior cloning all you need? understanding horizon in imitation learning." arXiv preprint arXiv:2407.15007 (2024).

  [2024]

- **zhen20243d:** Zhen, Haoyu, Qiu, Xiaowen, Chen, Peihao, Yang, Jincheng, Yan, Xin, Du, Yilun, Hong, Yining, Gan, Chuang.<br />
  "3d-vla: A 3d vision-language-action generative world model." arXiv preprint arXiv:2403.09631 (2024).

  [2024]

- **fan2024language:** Fan, Lingling, Chen, Kang, Xu, Zhezhuang, Yuan, Meng, Huang, Ping, Huang, Weibing.<br />
  "Language Reasoning in Vision-Language-Action Model for Robotic Grasping." 2024 China Automation Congress (CAC) (2024).

  [2024]

- **liuself:** Liu, Jiaming, Li, Chenxuan, Wang, Guanqun, Li, Xiaoqi, Chen, Sixiang, Xiong, Chuyan, Ge, Jiaxin, Zhou, Kaichen, Zhang, Shanghang.<br />
  "Self-Corrected Multimodal Large Language Model for Robot Manipulation and Reflection." Openreview (2024).

  [2024]

- **zhang2024learning:** Zhang, Kaifeng, Yin, Zhao-Heng, Ye, Weirui, Gao, Yang.<br />
  "Learning Manipulation Skills through Robot Chain-of-Thought with Sparse Failure Guidance." arXiv preprint arXiv:2405.13573 (2024).

  [2024]

- **zawalski2024robotic:** Zawalski, Micha{\l}, Chen, William, Pertsch, Karl, Mees, Oier, Finn, Chelsea, Levine, Sergey.<br />
  "Robotic control via embodied chain-of-thought reasoning." arXiv preprint arXiv:2407.08693 (2024).

  [2024]

- **duan2024aha:** Duan, Jiafei, Pumacay, Wilbert, Kumar, Nishanth, Wang, Yi Ru, Tian, Shulin, Yuan, Wentao, Krishna, Ranjay, Fox, Dieter, Mandlekar, Ajay, Guo, Yijie.<br />
  "AHA: A vision-language-model for detecting and reasoning over failures in robotic manipulation." arXiv preprint arXiv:2410.00371 (2024).

  [2024]

- **schmidgall2024gp:** Schmidgall, Samuel, Cho, Joseph, Zakka, Cyril, Hiesinger, William.<br />
  "Gp-vls: A general-purpose vision language model for surgery." arXiv preprint arXiv:2407.19305 (2024).

  [2024]

- **verbaan2024perception:** Verbaan, LE.<br />
  "Perception and Control with Large Language Models in Robotic Manipulation." TU Delft Library (2024).

  [2024]

- **wang2024non:** Wang, Hongqiu, Xing, Zhaohu, Wu, Weitong, Yang, Yijun, Tang, Qingqing, Zhang, Meixia, Xu, Yanwu, Zhu, Lei.<br />
  "Non-Invasive to Invasive: Enhancing FFA Synthesis from CFP with a Benchmark Dataset and a Novel Network." Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine (2024).

  [2024]

- **li2024llava:** Li, Jiajie, Skinner, Garrett, Yang, Gene, Quaranto, Brian R, Schwaitzberg, Steven D, Kim, Peter CW, Xiong, Jinjun.<br />
  "LLaVA-Surg: towards multimodal surgical assistant via structured surgical video learning." arXiv preprint arXiv:2408.07981 (2024).

  [2024]

- **trivedi2024explainable:** Trivedi, Chandan, Bhattacharya, Pronaya, Prasad, Vivek Kumar, Patel, Viraj, Singh, Arunendra, Tanwar, Sudeep, Sharma, Ravi, Aluvala, Srinivas, Pau, Giovanni, Sharma, Gulshan.<br />
  "Explainable AI for Industry 5.0: vision, architecture, and potential directions." IEEE Open Journal of Industry Applications (2024).

  [2024]

- **zhang2024vla:** Zhang, Haochen, Zantout, Nader, Kachana, Pujith, Wu, Zongyuan, Zhang, Ji, Wang, Wenshan.<br />
  "VLA-3D: A dataset for 3D semantic scene understanding and navigation." arXiv preprint arXiv:2411.03540 (2024).

  [2024]

- **ayaz2024medvlm:** Ayaz, Muhammad, Khan, Mustaqeem, Saqib, Muhammad, Khelifi, Adel, Sajjad, Muhammad, Elsaddik, Abdulmotaleb.<br />
  "MedVLM: Medical Vision-Language Model for Consumer Devices." IEEE Consumer Electronics Magazine (2024).

  [2024]

- **wang2024surgical:** Wang, Guankun, Bai, Long, Nah, Wan Jun, Wang, Jie, Zhang, Zhaoxi, Chen, Zhen, Wu, Jinlin, Islam, Mobarakol, Liu, Hongbin, Ren, Hongliang.<br />
  "Surgical-lvlm: Learning to adapt large vision-language model for grounded visual question answering in robotic surgery." arXiv preprint arXiv:2405.10948 (2024).

  [2024]

- **tang2024source:** Tang, Song, Su, Wenxin, Ye, Mao, Zhu, Xiatian.<br />
  "Source-free domain adaptation with frozen multimodal foundation model." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024).

  [2024]

- **xu2024mlevlm:** Xu, Dexuan, Chen, Yanyuan, Wang, Jieyi, Huang, Yue, Wang, Hanpin, Jin, Zhi, Wang, Hongxing, Yue, Weihua, He, Jing, Li, Hang, others.<br />
  "Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering." Findings of the Association for Computational Linguistics ACL 2024 (2024).

  [2024]

- **guruprasad2024benchmarking:** Guruprasad, Pranav, Sikka, Harshvardhan, Song, Jaewoo, Wang, Yangyue, Liang, Paul Pu.<br />
  "Benchmarking Vision, Language, \& Action Models on Robotic Learning Tasks." arXiv preprint arXiv:2411.05821 (2024).

  [2024]

- **wu2024smart:** Wu, Wei, Feng, Xiaoxin, Gao, Ziyan, Kan, Yuheng.<br />
  "SMART: scalable multi-agent real-time motion generation via next-token prediction." Advances in Neural Information Processing Systems (2024).

  [2024]

- **bathula2024policy:** Bathula, Naga Vardhani, Paleti, Indhu, Pagidi, Sravani, Akkumahanthi, Sahithi Sreya, Guduru, Naga Tejaswi.<br />
  "Policy Learning-Based Image Captioning With Vision Transformer." 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS) (2024).

  [2024]

- **haldar2024baku:** Haldar, Siddhant, Peng, Zhuoran, Pinto, Lerrel.<br />
  "Baku: An efficient transformer for multi-task policy learning." arXiv preprint arXiv:2406.07539 (2024).

  [2024]

- **chen2024augmented:** Chen, Haosen, Hou, Lei, Wu, Shaoze, Zhang, Guomin, Zou, Yang, Moon, Sungkon, Bhuiyan, Muhammed.<br />
  "Augmented reality, deep learning and vision-language query system for construction worker safety." Automation in Construction (2024).

  [2024]

- **geens2024bringing:** Geens, Robin.<br />
  "Bringing generative AI to edge devices through interoperable compute cores." Flanders AI Research Day, Location: Ghent (2024).

  [2024]

- **wang2024exploring:** Wang, Taowen, Han, Cheng, Liang, James Chenhao, Yang, Wenhao, Liu, Dongfang, Zhang, Luna Xinyu, Wang, Qifan, Luo, Jiebo, Tang, Ruixiang.<br />
  "Exploring the adversarial vulnerabilities of vision-language-action models in robotics." arXiv preprint arXiv:2411.13587 (2024).

  [2024]

- **jiang2024solami:** Jiang, Jianping, Xiao, Weiye, Lin, Zhengyu, Zhang, Huaizhong, Ren, Tianxiang, Gao, Yang, Lin, Zhiqian, Cai, Zhongang, Yang, Lei, Liu, Ziwei.<br />
  "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters." arXiv preprint arXiv:2412.00174 (2024).

  [2024]

- **ye2024latent:** Ye, Seonghyeon, Jang, Joel, Jeon, Byeongguk, Joo, Sejune, Yang, Jianwei, Peng, Baolin, Mandlekar, Ajay, Tan, Reuben, Chao, Yu-Wei, Lin, Bill Yuchen, others.<br />
  "Latent action pretraining from videos." arXiv preprint arXiv:2410.11758 (2024).

  [2024]

- **liu2024synthvlm:** Liu, Zheng, Liang, Hao, Huang, Xijie, Xiong, Wentao, Yu, Qinhan, Sun, Linzhuang, Chen, Chong, He, Conghui, Cui, Bin, Zhang, Wentao.<br />
  "Synthvlm: High-efficiency and high-quality synthetic data for vision language models." arXiv preprint arXiv:2407.20756 (2024).

  [2024]

- **cheng2024manipulation:** Cheng, Hao, Xiao, Erjia, Yu, Chengyuan, Yao, Zhao, Cao, Jiahang, Zhang, Qiang, Wang, Jiaxu, Sun, Mengshu, Xu, Kaidi, Gu, Jindong, others.<br />
  "Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models." arXiv preprint arXiv:2409.13174 (2024).

  [2024]

- **szot2024grounding:** Szot, Andrew, Mazoure, Bogdan, Agrawal, Harsh, Hjelm, R Devon, Kira, Zsolt, Toshev, Alexander.<br />
  "Grounding multimodal large language models in actions." Advances in Neural Information Processing Systems (2024).

  [2024]

- **kelly2024visiongpt:** Kelly, Chris, Hu, Luhui, Yang, Bang, Tian, Yu, Yang, Deshun, Yang, Cindy, Huang, Zaoshan, Li, Zihao, Hu, Jiayin, Zou, Yuexian.<br />
  "Visiongpt: Vision-language understanding agent using generalized multimodal framework." arXiv preprint arXiv:2403.09027 (2024).

  [2024]

- **torres2024comprehensive:** Torres, Nicol{\'a}s, Ulloa, Catalina, Araya, Ignacio, Ayala, Mat{\'\i}as, Jara, Sebasti{\'a}n.<br />
  "A comprehensive analysis of gender, racial, and prompt-induced biases in large language models." International Journal of Data Science and Analytics (2024).

  [2024]

- **lyre2024understandingaisemanticgrounding:** Holger Lyre.<br />
  ""Understanding AI": Semantic Grounding in Large Language Models." Unknown Venue (2024).

  [2024]

- **3dgsdet:** Yang Cao, Yuanliang Ju, Dan Xu.<br />
  "3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection." arXiv preprint arXiv:2410.01647 (2024).

  [2024]

- **cao2024collaborative:** Yang Cao, Yihan Zeng, Hang Xu, Dan Xu.<br />
  "Collaborative Novel Object Discovery and Box-Guided Cross-Modal Alignment for Open-Vocabulary 3D Object Detection." arXiv preprint arXiv:2406.00830 (2024).

  [2024]

- **Zhu2024UNIT:** Zhu, Yi, Zhou, Yanpeng, Wang, Chunwei, Cao, Yang, Han, Jianhua, Hou, Lu, Xu, Hang..<br />
  "UNIT: Unifying Image and Text Recognition in One Vision Encoder." NeurIPS (2024).

  [2024]

- **chen2024internvl:** Chen, Zhe, Wu, Jiannan, Wang, Wenhai, Su, Weijie, Chen, Guo, Xing, Sen, Zhong, Muyan, Zhang, Qinglong, Zhu, Xizhou, Lu, Lewei, others.<br />
  "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024).

  [2024]

