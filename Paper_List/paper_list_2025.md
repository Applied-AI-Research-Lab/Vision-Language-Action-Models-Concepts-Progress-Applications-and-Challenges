# Paper List 2025

### Follow-up Papers

- **sapkota2025review:** Sapkota, Ranjan, Roumeliotis, Konstantinos I, Cheppally, Rahul Harsha, Calero, Marco Flores, Karkee, Manoj.<br />
  "A Review of 3D Object Detection with Vision-Language Models." arXiv preprint arXiv:2504.18738 (2025).

  [2025]

- **li2025benchmark:** Li, Zongxia, Wu, Xiyang, Du, Hongyang, Nghiem, Huy, Shi, Guangyao.<br />
  "Benchmark evaluations, applications, and challenges of large vision language models: A survey." arXiv preprint arXiv:2501.02189 (2025).

  [2025]

- **yu2025survey:** Yu, Zhaoshu, Wang, Bo, Zeng, Pengpeng, Zhang, Haonan, Zhang, Ji, Gao, Lianli, Song, Jingkuan, Sebe, Nicu, Shen, Heng Tao.<br />
  "A survey on efficient vision-language-action models." arXiv preprint arXiv:2510.24795 (2025).

  [2025]

- **sapkota2025object:** Sapkota, Ranjan, Karkee, Manoj.<br />
  "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review." Available at SSRN 5233953 (2025).

  [2025]

- **pertsch2025fast:** Pertsch, Karl, Stachowicz, Kyle, Ichter, Brian, Driess, Danny, Nair, Suraj, Vuong, Quan, Mees, Oier, Finn, Chelsea, Levine, Sergey.<br />
  "Fast: Efficient action tokenization for vision-language-action models." arXiv preprint arXiv:2501.09747 (2025).

  [2025]

- **kim2025fine:** Kim, Moo Jin, Finn, Chelsea, Liang, Percy.<br />
  "Fine-tuning vision-language-action models: Optimizing speed and success." arXiv preprint arXiv:2502.19645 (2025).

  [2025]

- **bjorck2025gr00t:** Bjorck, Johan, Casta{\~n}eda, Fernando, Cherniadev, Nikita, Da, Xingye, Ding, Runyu, Fan, Linxi, Fang, Yu, Fox, Dieter, Hu, Fengyuan, Huang, Spencer, others.<br />
  "Gr00t n1: An open foundation model for generalist humanoid robots." arXiv preprint arXiv:2503.14734 (2025).

  [2025]

- **asuzu2025human:** Asuzu, Kosi, Singh, Harjinder, Idrissi, Moad.<br />
  "Human--robot interaction through joint robot planning with large language models." Intelligent Service Robotics (2025).

  [2025]

- **wu2025momanipvla:** Wu, Zhenyu, Zhou, Yuheng, Xu, Xiuwei, Wang, Ziwei, Yan, Haibin.<br />
  "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation." arXiv preprint arXiv:2503.13446 (2025).

  [2025]

- **xu2025vla:** Xu, Siyu, Wang, Yunke, Xia, Chenghao, Zhu, Dihao, Huang, Tao, Xu, Chang.<br />
  "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation." arXiv preprint arXiv:2502.02175 (2025).

  [2025]

- **wen2025tinyvla:** Wen, Junjie, Zhu, Yichen, Li, Jinming, Zhu, Minjie, Tang, Zhibin, Wu, Kun, Xu, Zhiyuan, Liu, Ning, Cheng, Ran, Shen, Chaomin, others.<br />
  "Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation." IEEE Robotics and Automation Letters (2025).

  [2025]

- **arai2025covla:** Arai, Hidehisa, Miwa, Keita, Sasaki, Kento, Watanabe, Kohei, Yamaguchi, Yu, Aoki, Shunsuke, Yamamoto, Issei.<br />
  "Covla: Comprehensive vision-language-action dataset for autonomous driving." 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2025).

  [2025]

- **zhou2025opendrivevla:** Zhou, Xingcheng, Han, Xuyuan, Yang, Feng, Ma, Yunpu, Knoll, Alois C.<br />
  "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model." arXiv preprint arXiv:2503.23463 (2025).

  [2025]

- **fu2025orion:** Fu, Haoyu, Zhang, Diankun, Zhao, Zongchuang, Cui, Jianfeng, Liang, Dingkang, Zhang, Chong, Zhang, Dingyuan, Xie, Hongwei, Wang, Bing, Bai, Xiang.<br />
  "ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation." arXiv preprint arXiv:2503.19755 (2025).

  [2025]

- **sautenkov2025uav:** Sautenkov, Oleg, Yaqoot, Yasheerah, Lykov, Artem, Mustafa, Muhammad Ahsan, Tadevosyan, Grik, Akhmetkazy, Aibek, Cabrera, Miguel Altamirano, Martynov, Mikhail, Karaf, Sausar, Tsetserukou, Dzmitry.<br />
  "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation." arXiv preprint arXiv:2501.05014 (2025).

  [2025]

- **zhou2025chatvla:** Zhou, Zhongyi, Zhu, Yichen, Zhu, Minjie, Wen, Junjie, Liu, Ning, Xu, Zhiyuan, Meng, Weibin, Cheng, Ran, Peng, Yaxin, Shen, Chaomin, others.<br />
  "Chatvla: Unified multimodal understanding and robot control with vision-language-action model." arXiv preprint arXiv:2502.14420 (2025).

  [2025]

- **huang2025otter:** Huang, Huang, Liu, Fangchen, Fu, Letian, Wu, Tingfan, Mukadam, Mustafa, Malik, Jitendra, Goldberg, Ken, Abbeel, Pieter.<br />
  "Otter: A vision-language-action model with text-aware visual feature extraction." arXiv preprint arXiv:2503.03734 (2025).

  [2025]

- **li2025pointvla:** Li, Chengmeng, Wen, Junjie, Peng, Yan, Peng, Yaxin, Feng, Feifei, Zhu, Yichen.<br />
  "PointVLA: Injecting the 3D World into Vision-Language-Action Models." arXiv preprint arXiv:2503.07511 (2025).

  [2025]

- **chen2025combatvla:** Chen, Peng, Bu, Pi, Wang, Yingyao, Wang, Xinyi, Wang, Ziming, Guo, Jie, Zhao, Yingxiu, Zhu, Qi, Song, Jun, Yang, Siran, others.<br />
  "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games." arXiv preprint arXiv:2503.09527 (2025).

  [2025]

- **liu2025hybridvla:** Liu, Jiaming, Chen, Hao, An, Pengju, Liu, Zhuoyang, Zhang, Renrui, Gu, Chenyang, Li, Xiaoqi, Guo, Ziyu, Chen, Sixiang, Liu, Mengzhen, others.<br />
  "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model." arXiv preprint arXiv:2503.10631 (2025).

  [2025]

- **hung2025nora:** Hung, Chia-Yu, Sun, Qi, Hong, Pengfei, Zadeh, Amir, Li, Chuan, Tan, U, Majumder, Navonil, Poria, Soujanya, others.<br />
  "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks." arXiv preprint arXiv:2504.19854 (2025).

  [2025]

- **qu2025spatialvla:** Qu, Delin, Song, Haoming, Chen, Qizhi, Yao, Yuanqi, Ye, Xinyi, Ding, Yan, Wang, Zhigang, Gu, JiaYuan, Zhao, Bin, Wang, Dong, others.<br />
  "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model." arXiv preprint arXiv:2501.15830 (2025).

  [2025]

- **zhang2025mole:** Zhang, Rongyu, Dong, Menghang, Zhang, Yuan, Heng, Liang, Chi, Xiaowei, Dai, Gaole, Du, Li, Wang, Dan, Du, Yuan, Zhang, Shanghang.<br />
  "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation." arXiv preprint arXiv:2503.20384 (2025).

  [2025]

- **li2025jarvis:** Li, Muyao, Wang, Zihao, He, Kaichen, Ma, Xiaojian, Liang, Yitao.<br />
  "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse." arXiv preprint arXiv:2503.16365 (2025).

  [2025]

- **zhang2025up:** Zhang, Jianke, Guo, Yanjiang, Hu, Yucheng, Chen, Xiaoyu, Zhu, Xiang, Chen, Jianyu.<br />
  "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent." arXiv preprint arXiv:2501.18867 (2025).

  [2025]

- **khan2025shake:** Khan, Muhamamd Haris, Asfaw, Selamawit, Iarchuk, Dmitrii, Cabrera, Miguel Altamirano, Moreno, Luis, Tokmurziyev, Issatay, Tsetserukou, Dzmitry.<br />
  "Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing." arXiv preprint arXiv:2501.06919 (2025).

  [2025]

- **zhao2025more:** Zhao, Han, Song, Wenxuan, Wang, Donglin, Tong, Xinyang, Ding, Pengxiang, Cheng, Xuelian, Ge, Zongyuan.<br />
  "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models." arXiv preprint arXiv:2503.08007 (2025).

  [2025]

- **zhong2025dexgraspvla:** Zhong, Yifan, Huang, Xuchuan, Li, Ruochong, Zhang, Ceyao, Liang, Yitao, Yang, Yaodong, Chen, Yuanpei.<br />
  "DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping." arXiv preprint arXiv:2502.20900 (2025).

  [2025]

- **wen2025dexvla:** Wen, Junjie, Zhu, Yichen, Li, Jinming, Tang, Zhibin, Shen, Chaomin, Feng, Feifei.<br />
  "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control." arXiv preprint arXiv:2502.05855 (2025).

  [2025]

- **ding2025humanoid:** Ding, Pengxiang, Ma, Jianfei, Tong, Xinyang, Zou, Binghong, Luo, Xinxin, Fan, Yiguo, Wang, Ting, Lu, Hongchao, Mo, Panzhong, Liu, Jinxin, others.<br />
  "Humanoid-VLA: Towards universal humanoid control with visual integration." arXiv preprint arXiv:2502.14795 (2025).

  [2025]

- **zhu2025objectvla:** Zhu, Minjie, Zhu, Yichen, Li, Jinming, Zhou, Zhongyi, Wen, Junjie, Liu, Xiaoyu, Shen, Chaomin, Peng, Yaxin, Feng, Feifei.<br />
  "ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration." arXiv preprint arXiv:2502.19250 (2025).

  [2025]

- **chen2025conrft:** Chen, Yuhui, Tian, Shuai, Liu, Shugao, Zhou, Yingting, Li, Haoran, Zhao, Dongbin.<br />
  "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy." arXiv preprint arXiv:2502.05450 (2025).

  [2025]

- **shi2025hi:** Shi, Lucy Xiaoyang, Ichter, Brian, Equi, Michael, Ke, Liyiming, Pertsch, Karl, Vuong, Quan, Tanner, James, Walling, Anna, Wang, Haohuan, Fusai, Niccolo, others.<br />
  "Hi robot: Open-ended instruction following with hierarchical vision-language-action models." arXiv preprint arXiv:2502.19417 (2025).

  [2025]

- **hao2025tla:** Hao, Peng, Zhang, Chaofan, Li, Dingzhe, Cao, Xiaoge, Hao, Xiaoshuai, Cui, Shaowei, Wang, Shuo.<br />
  "Tla: Tactile-language-action model for contact-rich manipulation." arXiv preprint arXiv:2503.08548 (2025).

  [2025]

- **guan2025efficient:** Guan, Weifan, Hu, Qinghao, Li, Aosheng, Cheng, Jian.<br />
  "Efficient vision-language-action models for embodied manipulation: A systematic survey." arXiv preprint arXiv:2510.17111 (2025).

  [2025]

- **zhang2025pure:** Zhang, Dapeng, Sun, Jing, Hu, Chenghui, Wu, Xiaoyan, Yuan, Zhenlong, Zhou, Rui, Shen, Fei, Zhou, Qingguo.<br />
  "Pure vision language action (vla) models: A comprehensive survey." arXiv preprint arXiv:2509.19012 (2025).

  [2025]

- **din2025vision:** Din, Muhayy Ud, Akram, Waseem, Saoud, Lyes Saad, Rosell, Jan, Hussain, Irfan.<br />
  "Vision language action models in robotic manipulation: A systematic review." arXiv preprint arXiv:2507.10672 (2025).

  [2025]

- **hou2025dita:** Hou, Zhi, Zhang, Tianyi, Xiong, Yuwen, Duan, Haonan, Pu, Hengjun, Tong, Ronglei, Zhao, Chengyang, Zhu, Xizhou, Qiao, Yu, Dai, Jifeng, others.<br />
  "Dita: Scaling diffusion transformer for generalist vision-language-action policy." arXiv preprint arXiv:2503.19757 (2025).

  [2025]

- **serpiva2025racevla:** Serpiva, Valerii, Lykov, Artem, Myshlyaev, Artyom, Khan, Muhammad Haris, Abdulkarim, Ali Alridha, Sautenkov, Oleg, Tsetserukou, Dzmitry.<br />
  "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour." arXiv preprint arXiv:2503.02572 (2025).

  [2025]

- **zhang2025safevla:** Zhang, Borong, Zhang, Yuhao, Ji, Jiaming, Lei, Yingshan, Dai, Josef, Chen, Yuanpei, Yang, Yaodong.<br />
  "Safevla: Towards safety alignment of vision-language-action model via safe reinforcement learning." arXiv preprint arXiv:2503.03480 (2025).

  [2025]

- **rawal2025intelligent:** Rawal, Parth Kapil.<br />
  "An Intelligent Versatile Pipeline for 6D Localization of Industrial Components in a Production Environment." Unknown Venue (2025).

  [2025]

- **zhao2025cot:** Zhao, Qingqing, Lu, Yao, Kim, Moo Jin, Fu, Zipeng, Zhang, Zhuoyang, Wu, Yecheng, Li, Zhaoshuo, Ma, Qianli, Han, Song, Finn, Chelsea, others.<br />
  "Cot-vla: Visual chain-of-thought reasoning for vision-language-action models." arXiv preprint arXiv:2503.22020 (2025).

  [2025]

- **li2025visual:** Li, Yifan, Lai, Zhixin, Bao, Wentao, Tan, Zhen, Dao, Anh, Sui, Kewei, Shen, Jiayi, Liu, Dong, Liu, Huan, Kong, Yu.<br />
  "Visual Large Language Models for Generalized and Specialized Applications." arXiv preprint arXiv:2501.02765 (2025).

  [2025]

- **sun2025review:** Sun, Jianghao, Mao, Pengjun, Kong, Lingju, Wang, Jun.<br />
  "A Review of Embodied Grasping." Sensors (Basel, Switzerland) (2025).

  [2025]

- **imran2025foundation:** Imran, Alishba, Gopalakrishnan, Keerthana.<br />
  "Foundation Models in Robotics." AI for Robotics: Toward Embodied and General Intelligence in the Physical World (2025).

  [2025]

- **gu2025humanoid:** Gu, Zhaoyuan, Li, Junheng, Shen, Wenlan, Yu, Wenhao, Xie, Zhaoming, McCrory, Stephen, Cheng, Xianyi, Shamsah, Abdulaziz, Griffin, Robert, Liu, C Karen, others.<br />
  "Humanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning." arXiv preprint arXiv:2501.02116 (2025).

  [2025]

- **song2025accelerating:** Song, Wenxuan, Chen, Jiayi, Ding, Pengxiang, Zhao, Han, Zhao, Wei, Zhong, Zhide, Ge, Zongyuan, Ma, Jun, Li, Haoang.<br />
  "Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding." arXiv preprint arXiv:2503.02310 (2025).

  [2025]

- **bartoccioni2025vavim:** Bartoccioni, Florent, Ramzi, Elias, Besnier, Victor, Venkataramanan, Shashanka, Vu, Tuan-Hung, Xu, Yihong, Chambon, Loick, Gidaris, Spyros, Odabas, Serkan, Hurych, David, others.<br />
  "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling." arXiv preprint arXiv:2502.15672 (2025).

  [2025]

- **huang2025decision:** Huang, Wei, Gu, Qinying, Ye, Nanyang.<br />
  "Decision SpikeFormer: Spike-Driven Transformer for Decision Making." arXiv preprint arXiv:2504.03800 (2025).

  [2025]

- **zhang2025generative:** Zhang, Kun, Yun, Peng, Cen, Jun, Cai, Junhao, Zhu, Didi, Yuan, Hangjie, Zhao, Chao, Feng, Tao, Wang, Michael Yu, Chen, Qifeng, others.<br />
  "Generative artificial intelligence in robotic manipulation: A survey." arXiv preprint arXiv:2503.03464 (2025).

  [2025]

- **lu2025probing:** Lu, Hong, Li, Hengxu, Shahani, Prithviraj Singh, Herbers, Stephanie, Scheutz, Matthias.<br />
  "Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture." arXiv preprint arXiv:2502.04558 (2025).

  [2025]

- **zhang2025gevrm:** Zhang, Hongyin, Ding, Pengxiang, Lyu, Shangke, Peng, Ying, Wang, Donglin.<br />
  "GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation." arXiv preprint arXiv:2502.09268 (2025).

  [2025]

- **lyu2025dywa:** Lyu, Jiangran, Li, Ziming, Shi, Xuesong, Xu, Chaoyi, Wang, Yizhou, Wang, He.<br />
  "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation." arXiv preprint arXiv:2503.16806 (2025).

  [2025]

- **xu2025embodied:** Xu, Jing, Sun, Qiyu, Han, Qing-Long, Tang, Yang.<br />
  "When embodied AI meets industry 5.0: human-centered smart manufacturing." IEEE/CAA Journal of Automatica Sinica (2025).

  [2025]

- **chen2025vision:** Chen, Xiaoyu, Xu, Wanru, Kan, Shichao, Zhang, Linna, Jin, Yi, Cen, Yigang, Li, Yidong.<br />
  "Vision-Semantics-Label: A New Two-step Paradigm for Action Recognition with Large Language Model." IEEE Transactions on Circuits and Systems for Video Technology (2025).

  [2025]

- **chen2025language:** Chen, Haoyang, Liu, Bo, Wang, Shuyue, Wang, Xiaosha, Han, Wenjuan, Zhu, Yixin, Wang, Xiaochun, Bi, Yanchao.<br />
  "Language modulates vision: Evidence from neural networks and human brain-lesion models." arXiv preprint arXiv:2501.13628 (2025).

  [2025]

- **bolya2025perception:** Bolya, Daniel, Huang, Po-Yao, Sun, Peize, Cho, Jang Hyun, Madotto, Andrea, Wei, Chen, Ma, Tengyu, Zhi, Jiale, Rajasegaran, Jathushan, Rasheed, Hanoona, others.<br />
  "Perception Encoder: The best visual embeddings are not at the output of the network." arXiv preprint arXiv:2504.13181 (2025).

  [2025]

- **dang2025ecbench:** Dang, Ronghao, Yuan, Yuqian, Zhang, Wenqi, Xin, Yifei, Zhang, Boqiang, Li, Long, Wang, Liuyi, Zeng, Qinyang, Li, Xin, Bing, Lidong.<br />
  "ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark." arXiv preprint arXiv:2501.05031 (2025).

  [2025]

- **wang2025learn:** Wang, Juan, Guo, Di, Liu, Huaping.<br />
  "Where To Learn: Embodied Perception Learning Planned by Vision-Language Models." IEEE Transactions on Cognitive and Developmental Systems (2025).

  [2025]

- **guo2025improving:** Guo, Yanjiang, Zhang, Jianke, Chen, Xiaoyu, Ji, Xiang, Wang, Yen-Jen, Hu, Yucheng, Chen, Jianyu.<br />
  "Improving Vision-Language-Action Model with Online Reinforcement Learning." arXiv preprint arXiv:2501.16664 (2025).

  [2025]

- **wang2025roboflamingo:** Wang, Sheng.<br />
  "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation." arXiv preprint arXiv:2503.19510 (2025).

  [2025]

- **zheng2025universal:** Zheng, Jinliang, Li, Jianxiong, Liu, Dongxiu, Zheng, Yinan, Wang, Zhihao, Ou, Zhonghong, Liu, Yu, Liu, Jingjing, Zhang, Ya-Qin, Zhan, Xianyuan.<br />
  "Universal Actions for Enhanced Embodied Foundation Models." arXiv preprint arXiv:2501.10105 (2025).

  [2025]

- **yang2025fp3:** Yang, Rujia, Chen, Geng, Wen, Chuan, Gao, Yang.<br />
  "FP3: A 3D Foundation Policy for Robotic Manipulation." arXiv preprint arXiv:2503.08950 (2025).

  [2025]

- **chen2025human:** Chen, Hongpeng, Li, Shufei, Fan, Junming, Duan, Anqing, Yang, Chenguang, Navarro-Alarcon, David, Zheng, Pai.<br />
  "Human-in-the-Loop Robot Learning for Smart Manufacturing: A Human-Centric Perspective." IEEE Transactions on Automation Science and Engineering (2025).

  [2025]

- **assres2025state:** Assres, Gebremariam, Bhandari, Guru, Shalaginov, Andrii, Gronli, Tor-Morten, Ghinea, Gheorghita.<br />
  "State-of-the-Art and Challenges of Engineering ML-Enabled Software Systems in the Deep Learning Era." ACM Computing Surveys (2025).

  [2025]

- **asif2025rapid:** Asif, Seemal, Bueno, Mikel, Ferreira, Pedro, Anandan, Paul, Zhang, Ze, Yao, Yue, Ragunathan, Gautham, Tinkler, Lloyd, Sotoodeh-Bahraini, Masoud, Lohse, Niels, others.<br />
  "Rapid and automated configuration of robot manufacturing cells." Robotics and Computer-Integrated Manufacturing (2025).

  [2025]

- **li2025robotic:** Li, Yanbang, Gong, Ziyang, Li, Haoyang, Huang, Xiaoqi, Kang, Haolan, Bai, Guangping, Ma, Xianzheng.<br />
  "Robotic Visual Instruction." arXiv preprint arXiv:2505.00693 (2025).

  [2025]

- **gao2025taxonomy:** Gao, Jensen, Belkhale, Suneel, Dasari, Sudeep, Balakrishna, Ashwin, Shah, Dhruv, Sadigh, Dorsa.<br />
  "A taxonomy for evaluating generalist robot policies." arXiv preprint arXiv:2503.01238 (2025).

  [2025]

- **ding2025visual:** Ding, Di, Yao, Tianliang, Luo, Rong, Sun, Xusen.<br />
  "Visual Question Answering in Robotic Surgery: A Comprehensive Review." IEEE Access (2025).

  [2025]

- **liu2025survey:** Liu, Yihao, Cao, Xu, Chen, Tingting, Jiang, Yankai, You, Junjie, Wu, Minghua, Wang, Xiaosong, Feng, Mengling, Jin, Yaochu, Chen, Jintai.<br />
  "A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities." arXiv preprint arXiv:2501.07468 (2025).

  [2025]

- **wang2025multimodal:** Wang, Yaoting, Wu, Shengqiong, Zhang, Yuecheng, Yan, Shuicheng, Liu, Ziwei, Luo, Jiebo, Fei, Hao.<br />
  "Multimodal chain-of-thought reasoning: A comprehensive survey." arXiv preprint arXiv:2503.12605 (2025).

  [2025]

- **liu2025screens:** Liu, Yihao, Cao, Xu, Chen, Tingting, Jiang, Yankai, You, Junjie, Wu, Minghua, Wang, Xiaosong, Feng, Mengling, Jin, Yaochu, Chen, Jintai.<br />
  "From Screens to Scenes: A Survey of Embodied AI in Healthcare." arXiv preprint arXiv:2501.07468 (2025).

  [2025]

- **dong2025advances:** Dong, Hao, Liu, Moru, Zhou, Kaiyang, Chatzi, Eleni, Kannala, Juho, Stachniss, Cyrill, Fink, Olga.<br />
  "Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models." arXiv preprint arXiv:2501.18592 (2025).

  [2025]

- **hu2025vision:** Hu, Yongquan'Owen', Tang, Jingyu, Gong, Xinya, Zhou, Zhongyi, Zhang, Shuning, Elvitigala, Don Samitha, Mueller, Florian'Floyd', Hu, Wen, Quigley, Aaron J.<br />
  "Vision-Based Multimodal Interfaces: A Survey and Taxonomy for Enhanced Context-Aware System Design." arXiv preprint arXiv:2501.13443 (2025).

  [2025]

- **gao2025vision:** Gao, Bei, Liu, Yuefeng, Li, Yanli, Li, Hongmei, Li, Meirong, He, Wenli.<br />
  "A vision-language model for predicting potential distribution land of soybean double cropping." Frontiers in Environmental Science (2025).

  [2025]

- **ikeda2025marcer:** Ikeda, Bryce, Gramopadhye, Maitrey, Nekervis, LillyAnn, Szafir, Daniel.<br />
  "MARCER: Multimodal Augmented Reality for Composing and Executing Robot Tasks." 2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI) (2025).

  [2025]

- **xue2025reactive:** Xue, Han, Ren, Jieji, Chen, Wendi, Zhang, Gu, Fang, Yuan, Gu, Guoying, Xu, Huazhe, Lu, Cewu.<br />
  "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation." arXiv preprint arXiv:2503.02881 (2025).

  [2025]

- **pang2025towards:** Pang, Jiazhen, Zheng, Pai, Fan, Junming, Liu, Tianyuan.<br />
  "Towards cognition-augmented human-centric assembly: A visual computation perspective." Robotics and Computer-Integrated Manufacturing (2025).

  [2025]

- **rodriguez2025integrating:** Rodriguez-Juan, Javier, Ortiz-Perez, David, Garcia-Rodriguez, Jose, Tom{\'a}s, David, Nalepa, Grzegorz J.<br />
  "Integrating advanced vision-language models for context recognition in risks assessment." Neurocomputing (2025).

  [2025]

- **jiang2025behavior:** Jiang, Yunfan, Zhang, Ruohan, Wong, Josiah, Wang, Chen, Ze, Yanjie, Yin, Hang, Gokmen, Cem, Song, Shuran, Wu, Jiajun, Fei-Fei, Li.<br />
  "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities." arXiv preprint arXiv:2503.05652 (2025).

  [2025]

- **sahili2025scaling:** Sahili, Zahraa Al, Patras, Ioannis, Purver, Matthew.<br />
  "Scaling for Fairness? Analyzing Model Size, Data Composition, and Multilinguality in Vision-Language Bias." arXiv preprint arXiv:2501.13223 (2025).

  [2025]

- **fang2025rebot:** Fang, Yu, Yang, Yue, Zhu, Xinghao, Zheng, Kaiyuan, Bertasius, Gedas, Szafir, Daniel, Ding, Mingyu.<br />
  "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis." arXiv preprint arXiv:2503.14526 (2025).

  [2025]

- **sun2025prism:** Sun, Haowen, Wang, Han, Ma, Chengzhong, Zhang, Shaolong, Ye, Jiawei, Chen, Xingyu, Lan, Xuguang.<br />
  "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations." arXiv preprint arXiv:2504.20520 (2025).

  [2025]

- **li2025hamster:** Li, Yi, Deng, Yuquan, Zhang, Jesse, Jang, Joel, Memmel, Marius, Yu, Raymond, Garrett, Caelan Reed, Ramos, Fabio, Fox, Dieter, Li, Anqi, others.<br />
  "HAMSTER: Hierarchical action models for open-world robot manipulation." arXiv preprint arXiv:2502.05485 (2025).

  [2025]

- **intelligence2025pi:** Intelligence, Physical, Black, Kevin, Brown, Noah, Darpinian, James, Dhabalia, Karan, Driess, Danny, Esmail, Adnan, Equi, Michael, Finn, Chelsea, Fusai, Niccolo, others.<br />
  "pi0.5: a Vision-Language-Action Model with Open-World Generalization." arXiv preprint arXiv:2504.16054 (2025).

  [2025]

- **jones2025beyond:** Jones, Joshua, Mees, Oier, Sferrazza, Carmelo, Stachowicz, Kyle, Abbeel, Pieter, Levine, Sergey.<br />
  "Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding." arXiv preprint arXiv:2501.04693 (2025).

  [2025]

- **hong2025building:** Hong, Yining.<br />
  "Building 3D Foundation Models for the Embodied Minds." Unknown Venue (2025).

  [2025]

- **samson2025scalable:** Samson, Marie, Muraccioli, Bastien, Kanehiro, Fumio.<br />
  "Scalable, Training-Free Visual Language Robotics: a modular multi-model framework for consumer-grade GPUs." 2025 IEEE/SICE International Symposium on System Integration (SII) (2025).

  [2025]

- **song2025survey:** Song, Mingchen, Deng, Xiang, Zhou, Zhiling, Wei, Jie, Guan, Weili, Nie, Liqiang.<br />
  "A Survey on Diffusion Policy for Robotic Manipulation: Taxonomy, Analysis, and Future Directions." Authorea Preprints (2025).

  [2025]

- **noorani2025abstraction:** Noorani, Erfaun, Serlin, Zachary, Price, Ben, Velasquez, Alvaro.<br />
  "From Abstraction to Reality: DARPA's Vision for Robust Sim-to-Real Autonomy." arXiv preprint arXiv:2503.11007 (2025).

  [2025]

- **zhang2025slim:** Zhang, Haichao, Yu, Haonan, Zhao, Le, Choi, Andrew, Bai, Qinxun, Yang, Break, Xu, Wei.<br />
  "SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon Visuomotor Learning." arXiv preprint arXiv:2501.09905 (2025).

  [2025]

- **polubarov2025vintix:** Polubarov, Andrey, Lyubaykin, Nikita, Derevyagin, Alexander, Zisman, Ilya, Tarasov, Denis, Nikulin, Alexander, Kurenkov, Vladislav.<br />
  "Vintix: Action Model via In-Context Reinforcement Learning." arXiv preprint arXiv:2501.19400 (2025).

  [2025]

- **mumuni2025large:** Mumuni, Alhassan, Mumuni, Fuseini.<br />
  "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches." arXiv preprint arXiv:2501.03151 (2025).

  [2025]

- **raza2025responsible:** Raza, Shaina, Qureshi, Rizwan, Zahid, Anam, Fioresi, Joseph, Sadak, Ferhat, Saeed, Muhammad, Sapkota, Ranjan, Jain, Aditya, Zafar, Anas, Hassan, Muneeb Ul, others.<br />
  "Who is responsible? the data, models, users or regulations? responsible generative ai for a sustainable future." arXiv preprint arXiv:2502.08650 (2025).

  [2025]

- **team2025gemini:** Team, Gemini Robotics, Abeyruwan, Saminda, Ainslie, Joshua, Alayrac, Jean-Baptiste, Arenas, Montserrat Gonzalez, Armstrong, Travis, Balakrishna, Ashwin, Baruch, Robert, Bauza, Maria, Blokzijl, Michiel, others.<br />
  "Gemini robotics: Bringing ai into the physical world." arXiv preprint arXiv:2503.20020 (2025).

  [2025]

- **plaat2025agentic:** Plaat, Aske, van Duijn, Max, van Stein, Niki, Preuss, Mike, van der Putten, Peter, Batenburg, Kees Joost.<br />
  "Agentic Large Language Models, a survey." arXiv preprint arXiv:2503.23037 (2025).

  [2025]

- **sharshar2025vision:** Sharshar, Ahmed, Khan, Latif U, Ullah, Waseem, Guizani, Mohsen.<br />
  "Vision-Language Models for Edge Networks: A Comprehensive Survey." arXiv preprint arXiv:2502.07855 (2025).

  [2025]

- **fan2025interleavevlaenhancingrobotmanipulation:** Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding.<br />
  "Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions." Unknown Venue (2025).

  [2025]

- **deng2025graspvlagraspingfoundationmodel:** Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, He Wang.<br />
  "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data." Unknown Venue (2025).

  [2025]

- **xiang2025vla:** Xiang, Tian-Yu, Jin, Ao-Qun, Zhou, Xiao-Hu, Gui, Mei-Jiang, Xie, Xiao-Liang, Liu, Shi-Qi, Wang, Shuang-Yi, Duang, Sheng-Bin, Wang, Si-Cheng, Lei, Zheng, others.<br />
  "VLA Model-Expert Collaboration for Bi-directional Manipulation Learning." arXiv preprint arXiv:2503.04163 (2025).

  [2025]

- **shu2025large:** Shu, Dong, Zhao, Haiyan, Hu, Jingyu, Liu, Weiru, Payani, Ali, Cheng, Lu, Du, Mengnan.<br />
  "Large vision-language model alignment and misalignment: A survey through the lens of explainability." arXiv preprint arXiv:2501.01346 (2025).

  [2025]

- **Waite3722033:** Waite, Joshua R., Hasan, Md Zahid, Liu, Qisai, Jiang, Zhanhong, Hegde, Chinmay, Sarkar, Soumik.<br />
  "RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception." Proceedings of the ACM/IEEE 16th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2025) (2025).

  [2025]

- **liu2025codrivevlmvlmenhancedurbancooperative:** Haichao Liu, Ruoyu Yao, Wenru Liu, Zhenmin Huang, Shaojie Shen, Jun Ma.<br />
  "CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems." Unknown Venue (2025).

  [2025]

- **Qwen2.5-VL:** Bai, Shuai, Chen, Keqin, Liu, Xuejing, Wang, Jialin, Ge, Wenbin, Song, Sibo, Dang, Kai, Wang, Peng, Wang, Shijie, Tang, Jun, Zhong, Humen, Zhu, Yuanzhi, Yang, Mingkun, Li, Zhaohai, Wan, Jianqiang, Wang, Pengfei, Ding, Wei, Fu, Zheren, Xu, Yiheng, Ye, Jiabo, Zhang, Xi, Xie, Tianbao, Cheng, Zesen, Zhang, Hang, Yang, Zhibo, Xu, Haiyang, Lin, Junyang.<br />
  "Qwen2.5-VL Technical Report." arXiv preprint arXiv:2502.13923 (2025).

  [2025]

- **singh2025neural:** Singh, Gautam.<br />
  "Neural Object-Centric Scene Representation and Generation." Unknown Venue (2025).

  [2025]

- **zhou2025learning:** Zhou, Da-Wei, Zhang, Yuanhan, Wang, Yan, Ning, Jingyi, Ye, Han-Jia, Zhan, De-Chuan, Liu, Ziwei.<br />
  "Learning without forgetting for vision-language models." IEEE Transactions on Pattern Analysis and Machine Intelligence (2025).

  [2025]

- **yang2025recent:** Yang, Yutao, Zhou, Jie, Ding, Xuanwen, Huai, Tianyu, Liu, Shunyu, Chen, Qin, Xie, Yuan, He, Liang.<br />
  "Recent advances of foundation language models-based continual learning: A survey." ACM Computing Surveys (2025).

  [2025]

- **yang2025lohovla:** Yang, Yi, Sun, Jiaxuan, Kou, Siqi, Wang, Yihan, Deng, Zhijie.<br />
  "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks." arXiv preprint arXiv:2506.00411 (2025).

  [2025]

- **yang2025guiding:** Yang, Zhutian, Garrett, Caelan, Fox, Dieter, Lozano-P{\'e}rez, Tom{\'a}s, Kaelbling, Leslie Pack.<br />
  "Guiding long-horizon task and motion planning with vision language models." 2025 IEEE International Conference on Robotics and Automation (ICRA) (2025).

  [2025]

- **xu2025language:** Xu, Yichang, Liu, Gaowen, Kompella, Ramana Rao, Hu, Sihao, Huang, Tiansheng, Ilhan, Fatih, Tekin, Selim Furkan, Yahn, Zachary, Liu, Ling.<br />
  "Language-Vision Planner and Executor for Text-to-Visual Reasoning." arXiv preprint arXiv:2506.07778 (2025).

  [2025]

- **shivadekar2025artificial:** Shivadekar, Samit.<br />
  "Artificial Intelligence for Cognitive Systems: Deep Learning, Neuro-symbolic Integration, and Human-Centric Intelligence." Unknown Venue (2025).

  [2025]

- **schakkal2025hierarchical:** Schakkal, Andr{\'e}, Zandonati, Ben, Yang, Zhutian, Azizan, Navid.<br />
  "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation." arXiv preprint arXiv:2506.22827 (2025).

  [2025]

- **gao2025vla:** Gao, Chongkai, Liu, Zixuan, Chi, Zhenghao, Huang, Junshan, Fei, Xin, Hou, Yiwen, Zhang, Yuxuan, Lin, Yudi, Fang, Zhirui, Jiang, Zeyu, others.<br />
  "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models." arXiv preprint arXiv:2506.17561 (2025).

  [2025]

- **sanyal2025asma:** Sanyal, Sourav, Roy, Kaushik.<br />
  "Asma: An adaptive safety margin algorithm for vision-language drone navigation via scene-aware control barrier functions." IEEE Robotics and Automation Letters (2025).

  [2025]

- **shadab2025comparison:** Shadab Siddiqui, MA, Rabbi, MS, Islam, Md Jobayer, Ahmed, Radif Uddin.<br />
  "Comparison of Different Controller Architectures for Autonomous Driving and Recommendations for Robust and Safe Implementations." Journal of Advanced Transportation (2025).

  [2025]

- **huang2025building:** Huang, Yue, Hua, Hang, Zhou, Yujun, Jing, Pengcheng, Nagireddy, Manish, Padhi, Inkit, Dolcetti, Greta, Xu, Zhangchen, Chaudhury, Subhajit, Rawat, Ambrish, others.<br />
  "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data." arXiv preprint arXiv:2510.09781 (2025).

  [2025]

- **shukor2025smolvla:** Shukor, Mustafa, Aubakirova, Dana, Capuano, Francesco, Kooijmans, Pepijn, Palma, Steven, Zouitine, Adil, Aractingi, Michel, Pascal, Caroline, Russi, Martino, Marafioti, Andres, others.<br />
  "Smolvla: A vision-language-action model for affordable and efficient robotics." arXiv preprint arXiv:2506.01844 (2025).

  [2025]

- **ye2025token:** Ye, Yifan, Ma, Jiaqi, Cen, Jun, Lu, Zhihe.<br />
  "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models." arXiv preprint arXiv:2512.09927 (2025).

  [2025]

- **tan2025think:** Tan, Xudong, Yang, Yaoxin, Ye, Peng, Zheng, Jialin, Bai, Bizhe, Wang, Xinyi, Hao, Jia, Chen, Tao.<br />
  "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models." arXiv preprint arXiv:2505.21200 (2025).

  [2025]

- **xu2025stare:** Xu, Feng, Zhai, Guangyao, Kong, Xin, Fu, Tingzhong, Gordon, Daniel FN, An, Xueli, Busam, Benjamin.<br />
  "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models." arXiv preprint arXiv:2512.05107 (2025).

  [2025]

- **vinod2025sebvs:** Vinod, Krishna, Ramesh, Prithvi Jai, Chakravarthi, Bharatesh, others.<br />
  "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation." arXiv preprint arXiv:2508.17643 (2025).

  [2025]

- **wei2025focus:** Wei, Chiyue, Guo, Cong, Zhang, Junyao, Shan, Haoxuan, Xu, Yifan, Zhang, Ziyue, Liu, Yudong, Wang, Qinsi, Zhou, Changchun, Li, Hai, others.<br />
  "Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models." arXiv preprint arXiv:2512.14661 (2025).

  [2025]

- **fang2025sqap:** Fang, Hengyu, Liu, Yijiang, Du, Yuan, Du, Li, Yang, Huanrui.<br />
  "Sqap-vla: A synergistic quantization-aware pruning framework for high-performance vision-language-action models." arXiv preprint arXiv:2509.09090 (2025).

  [2025]

- **shao2025large:** Shao, Rui, Li, Wei, Zhang, Lingsen, Zhang, Renshan, Liu, Zhiyang, Chen, Ran, Nie, Liqiang.<br />
  "Large vlm-based vision-language-action models for robotic manipulation: A survey." arXiv preprint arXiv:2508.13073 (2025).

  [2025]

- **chi2025impromptu:** Chi, Haohan, Gao, Huan-ang, Liu, Ziming, Liu, Jianing, Liu, Chenyu, Li, Jinwei, Yang, Kaisen, Yu, Yangcheng, Wang, Zeda, Li, Wenyi, others.<br />
  "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models." arXiv preprint arXiv:2505.23757 (2025).

  [2025]

- **park2025saliency:** Park, Seongmin, Kim, Hyungmin, Kim, Sangwoo, Jeon, Wonseok, Yang, Juyoung, Jeon, Byeongwook, Oh, Yoonseon, Choi, Jungwook.<br />
  "Saliency-aware quantized imitation learning for efficient robotic control." Proceedings of the IEEE/CVF International Conference on Computer Vision (2025).

  [2025]

- **chang2025survey:** Chang, Ching, Shi, Yidan, Cao, Defu, Yang, Wei, Hwang, Jeehyun, Wang, Haixin, Pang, Jiacheng, Wang, Wei, Liu, Yan, Peng, Wen-Chih, others.<br />
  "A survey of reasoning and agentic systems in time series with large language models." arXiv preprint arXiv:2509.11575 (2025).

  [2025]

- **li2025seeing:** Li, Siyou, Wu, Huanan, Shao, Juexi, Ma, Yinghao, Gan, Yujian, Luo, Yihao, Wang, Yuwei, Nie, Dong, Wang, Lu, Wu, Wengqing, others.<br />
  "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models." arXiv preprint arXiv:2511.11910 (2025).

  [2025]

- **fan2025long:** Fan, Yiguo, Ding, Pengxiang, Bai, Shuanghao, Tong, Xinyang, Zhu, Yuyang, Lu, Hongchao, Dai, Fengqi, Zhao, Wei, Liu, Yang, Huang, Siteng, others.<br />
  "Long-vla: Unleashing long-horizon capability of vision language action model for robot manipulation." arXiv preprint arXiv:2508.19958 (2025).

  [2025]

- **koo2025retovla:** Koo, Jiyeon, Cho, Taewan, Kang, Hyunjoon, Pyo, Eunseom, Oh, Tae Gyun, Kim, Taeryang, Choi, Andrew Jaeyong.<br />
  "Retovla: Reusing register tokens for spatial reasoning in vision-language-action models." arXiv preprint arXiv:2509.21243 (2025).

  [2025]

- **yang2025vlaser:** Yang, Ganlin, Zhang, Tianyi, Hao, Haoran, Wang, Weiyun, Liu, Yibin, Wang, Dehui, Chen, Guanzhou, Cai, Zijian, Chen, Junting, Su, Weijie, others.<br />
  "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning." arXiv preprint arXiv:2510.11027 (2025).

  [2025]

- **liang2025discrete:** Liang, Zhixuan, Li, Yizhuo, Yang, Tianshuo, Wu, Chengyue, Mao, Sitong, Nian, Tian, Pei, Liuao, Zhou, Shunbo, Yang, Xiaokang, Pang, Jiangmiao, others.<br />
  "Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies." arXiv preprint arXiv:2508.20072 (2025).

  [2025]

- **luo2025being:** Luo, Hao, Feng, Yicheng, Zhang, Wanpeng, Zheng, Sipeng, Wang, Ye, Yuan, Haoqi, Liu, Jiazheng, Xu, Chaoyi, Jin, Qin, Lu, Zongqing.<br />
  "Being-h0: vision-language-action pretraining from large-scale human videos." arXiv preprint arXiv:2507.15597 (2025).

  [2025]

- **yang2025egovla:** Yang, Ruihan, Yu, Qinxi, Wu, Yecheng, Yan, Rui, Li, Borui, Cheng, An-Chieh, Zou, Xueyan, Fang, Yunhao, Cheng, Xuxin, Qiu, Ri-Zhao, others.<br />
  "Egovla: Learning vision-language-action models from egocentric human videos." arXiv preprint arXiv:2507.12440 (2025).

  [2025]

- **deng2025stereovla:** Deng, Shengliang, Yan, Mi, Zheng, Yixin, Su, Jiayi, Zhang, Wenhao, Zhao, Xiaoguang, Cui, Heming, Zhang, Zhizheng, Wang, He.<br />
  "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision." arXiv preprint arXiv:2512.21970 (2025).

  [2025]

- **sun2025geovla:** Sun, Lin, Xie, Bin, Liu, Yingfei, Shi, Hao, Wang, Tiancai, Cao, Jiale.<br />
  "Geovla: Empowering 3d representations in vision-language-action models." arXiv preprint arXiv:2508.09071 (2025).

  [2025]

- **yang2025efficientvla:** Yang, Yantai, Wang, Yuhao, Wen, Zichen, Zhongwei, Luo, Zou, Chang, Zhang, Zhipeng, Wen, Chuan, Zhang, Linfeng.<br />
  "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models." arXiv preprint arXiv:2506.10100 (2025).

  [2025]

